\chapter{Future Implementation}
When implementing this in the future there are a lot of things that should come into consideration. First off, the system architecture. We want the system to work as optimal as possible. Meaning that we would want the faster algorithms at the forefront doing the heavy load of the work before passing the results onto the slower algorithms. This means that we need to look into how many articles we can take out of the corpus by running the Cosine algorithm before sending the articles onto LCS. We could have several instances of this program to run on several machines so that we could split the work load down for each instance of the LCS algorithm should it prove to have issues with analysing the articles. For the relative small amounts of the articles from the test corpus that was tested in this thesis, the run time was acceptable, but how it scales into a larger version remains to be seen. There are factors to this, one being converting the strings to int, another being the whole segment of text preparation, but also deciding how many article comparisons should be done by LCS. Perhaps we do not need to test articles in LCS that have scored higher than 0.8 in Cosine, or lower than 0.5 in Cosine. This would be determined by more testing, and evaluation of the results. One thing is clear, we should strive towards having as few article comparisons sent to be checked by the slower algorithms as possible.

We should make sure that the algorithms implemented have various focus areas, so that we can cover multiple angles to doing text comparison (as seen in figure ~\ref{MultipleAlgo}). It should also be priority that algorithms only do a single task, and not try to do a full range of comparisons single handedly.

A lot of effort should also go into the preparation phase, where text is being normalized. We saw several examples of text normalization giving an inaccurate result, albeit only minor inaccuracies. However this error source might as well be eliminated to help get as an accurate score as possible.

Another thing to consider is the cost. A lot of time and manpower could be sunk into a task that would provide little value. There is a need to weigh cost and benefits for all extra implementation, but this is a talk for another time.

One should also keep in mind how the results from this is presented to the humans for manual evaluation. The Excel sheets created for this assignment is intended for testing purposes only, and was a valuable tool in visualising the results for this thesis, but perhaps not the best way of presenting results to humans, especially not if there is many comparisons to go through.

And finally a lot of testing of the score evaluation should be made. This part could end up being a total different method than what is currently is, by having many algorithms doing their separate scoring, but a lot of testing of this is still needed.

\section{Presentation to the Clients}
The idea is that all the work is done behind the scenes. A client would just be presented with the list of articles,   where comparisons would be grouped, with the longest one first and a comment that "these articles all are to a great extend the same as the first one". It would be of great value to Infomedia to hear from clients if they experience any sort of problems with this, like if articles are grouped, but unrelated.