\chapter{Discussion}

This project is dealing with a topic that is of big importance to the industry that Infomedia works in, and it has proven to be anything, but a simple task. While this thesis is only a prototype for a system that would include several algorithms to do text matching, a few defining results were made. However, there is still a lot of testing to be done in order to call the system waterproof. This chapter will look into some of the issues that were discovered along the way.

\section{Text Normalization and Scores}
As talked about in the beginning of this thesis, the issue of text normalization showed to have an impact. During the thesis we have seen several examples of the issues of not being able to do a full normalization of the article text before applying the algorithms. So far, no apparent issues have been found that includes special characters, but there have been a lot of formatting issues. We saw in figure ~\ref{JPPOLSencondHighestLCS} the problem with white spaces. This might not have been a deciding factor in most cases as to what match group have been applied to a comparison, but we cannot be sure.

Applying the match groups was based on several things, amongst that the LCS scores. This score is sensitive to formatting errors, and to eliminate this factor we would have to normalize the text.

\section{LCS Threshold}
In this thesis, we have been working with an LCS threshold of four. We saw at least one example of where this threshold had a great impact (see figure ~\ref{LengthIssue}). Another factor worth taking into account when looking at the LCS threshold would be to remove white spaces from the LCS. Also removing stop words would impact this.

\section{Score Evaluation}
Another area that really need some more thorough testing is the score evaluation. Both groups of enums created (the ones describing article length and the ones that are placing comparisons into match groups) are based on imperative studies. There is not set rules for when an article is to be considered "short" or "long". Neither are there any defining boundaries that tells us whether a comparison is a match or a partial match. All of this is based of "what felt right at the time, and the results found when testing". The results that have been found shows us, that these defining enums appears to be valid. We have seen a good few examples that proves that the combination of algorithms can in fact help give a better score indication. 

Especially the enum "SameContent" can prove to be a bit of a hassle. Imagine two articles dealing with the same person, but in different relations. As an example of this we could look at the ex-professional cyclist Bjarne Riis. He has been cycling for many years, won the Tour de France, admitted to doing drugs to help him do so, been a manager of a successful bike team, which then in turn also have been involved in doping scandals. In general the person Bjarne Riis would also be linked to ex-professional handballer Anne Dorthe Tanderup. So we can get a lot of different connections with Bjarne Riis. But when are we looking at two articles that essentially deal with the same topic? One article dealing with Bjarne could be about his days as a pro cyclist, but also about his drug use, another article could be dealing with him as the manager of his bike team which also have been involved in scandals regarding drug use. The two article would probably score pretty high with the Cosine algorithm as we would have a lot of words that are repeated in both articles, and which is not stop words, and then score low on the LCS, as the articles would have little text in common (in terms of substrings). That could indicate right off the bat that these two articles have the same topic. And to a certain degree they have, but to what extend? Are we satisfied with a match that evolves around the same general area (in the case Bjarne Riis), or do we want specifics? We do therefore need to look into this group of comparisons to decide what we will do down the line. This could be an area which another algorithm could cover.

The limiting factors that is time, and this thesis being done by one person, sets a limit as to how many comparisons can be proof read. There will undoubtedly be false positives in the result sets, that is the way of things with a prototype. It would require a lot more work to fully confirm if this solution is to be 100\% right all the time.

\section{Test Size}
Another issue brought onto this thesis by the limitations of time and personnel available, is the issue of how much material in the test corpus could be used. Of the 557 sources available in the test corpus, only four have been tested. Although the findings when testing these four sources was conclusive, in the sense that there was no comparisons that produced a result that could not be explained or that seemed too out of the ordinary, a test on a bigger part of the test corpus are really needed. This is also true for the way the score evaluation was made. The methods evaluating scores was based of the findings of comparing the four sources. And testing this on all 557 sources, would maybe alter the thresholds for weighing scores.

\section{Low Score Comparisons}
As we saw in figure ~\ref{LengthIssue} there are cases in which low score comparisons would provide interesting information. These might be rare and far in between, but we should still consider whether we should spend time and energy on finding these. One could argue that because these cases seems to be extremely rare (keeping in mind that we only tested few algorithms and sources), we could miss out on potential matches by dismissing these comparisons as no matches.

\section{The Issue of Matches}
Once all the data have been broken down and analysed, some where along the line someone will have to define the following: When is two articles a match. For better or for worse, this thesis is setting up a number of assumptions about when article are matching and when they are not, and as seen in the results there have been found a lot of good results. However as discussed earlier in this chapter there are still areas in which the algorithms falls short. They cannot decide when articles match without human hands to tell them when something is a match. This part of the problem falls to the "end users" of this product, in this case sales people or people with better lingual understanding that can help us set these defining boundaries that helps the algorithms when articles match. We need someone to tell the program when articles are defined as matching, how ever fussy that term may be. Having many algorithms that scores articles in different ways can tell us a wider story as too how well articles match, but the problem remains. In the end we will probably always have a number of matches that either are false positives or false negatives, so continuous verification of the results will be needed. With that comes adjusting the algorithms so that the next time, the results are some what better.

\section{Overall}
When all is said and done, this is after all, only a prototype. There is room for improvement and tweaking the way the program works, both in terms of coding but also in terms of scoring comparisons and text preparation. More testing would also be required in order to fully confirm that the way that the program works, actually produces true positive results. By and large the thesis did end up proving what we set out to do, that is

\begin{itemize}
	\item Analysed the currently implemented system and found out what was needed for picking a new algorithm.
	\item Implemented the algorithm and in combination with the existing algorithm scored the corpus.
	\item Went over the results produced by the two algorithms and verified or rejected their findings.
	\item Proved that we were indeed able to get a better and more detailed result set by using a combination of algorithms, instead of the result produced by just the one already implemented.
\end{itemize}